# ğŸ“ Content-Based Search in Multimedia Digital Archives using Artificial Intelligence

This repository is part of a university thesis project focused on **semantic search** within **multimedia digital 
archives** â€” primarily **images** and **PDF documents** â€” using **Artificial Intelligence** models such as **CLIP** 
and **M-CLIP (multilingual CLIP)**.

The system allows users to perform **text-to-image**, **image-to-image**, **text-to-PDF**, and **PDF-to-PDF** 
similarity searches through a unified Streamlit web interface.

---

## ğŸ“ Project Structure

```
content-search-ai/
â”œâ”€â”€ data/                   # Datasets (COCO images, PDFs, embeddings)
â”‚   â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ pdfs/
â”‚   â”œâ”€â”€ annotations/
â”‚   â””â”€â”€ embeddings/
â”œâ”€â”€ core/                   # Core logic (ImageSearcher, PDFSearcher)
â”œâ”€â”€ app.py                  # Streamlit web interface
â”œâ”€â”€ main.py                 # CLI execution script
â”œâ”€â”€ environment.yml         # Conda environment definition
â”œâ”€â”€ requirements.txt        # PIP installation file
â””â”€â”€ README.md               # Project documentation
```

---

## ğŸš€ Implemented Features

| Category                        | Description                                                       | Status        |
|---------------------------------|-------------------------------------------------------------------|---------------|
| ğŸ–¼ï¸ **Text â†’ Image Search**     | Search for similar images using a text prompt (via CLIP & M-CLIP) | âœ… Implemented |
| ğŸ–¼ï¸ **Image â†’ Image Search**    | Visual similarity detection using image embeddings                | âœ… Implemented |
| ğŸ“š **PDF â†’ PDF Search**         | Compare PDF documents on a per-page semantic level (M-CLIP)       | âœ… Implemented |
| ğŸ’¬ **Text â†’ PDF Search**        | Semantic text-to-document search inside PDF archives              | âœ… Implemented |
| ğŸ§ **Audio Phrase Search**      | Speech-to-text and semantic similarity using Whisper              | ğŸš§ Planned    |
| ğŸ¥ **Video Content Search**     | Frame-based and transcript-based video indexing                   | ğŸš§ Planned    |

---

## ğŸ§  Technologies Used

- **OpenAI CLIP / M-CLIP** â€“ for multilingual image-text embeddings  
- **Sentence Transformers (SBERT)** â€“ for PDF and text similarity  
- **PyTorch** â€“ model inference and tensor computation  
- **FAISS** â€“ vector-based similarity search  
- **Streamlit** â€“ interactive user interface  
- **Deep Translator** â€“ automatic language translation (Greek â†” English)  
- **PyMuPDF (fitz)** â€“ PDF parsing and text extraction  

---

## âš™ï¸ How It Works

The system computes embeddings for all supported media types and stores them in vector form (`.pt` files).  
When a new query (text, image, or PDF) is provided, the model encodes it into the same vector space and compares 
it against the stored embeddings using **cosine similarity**.

---

## ğŸ§© Execution Modes

### 1ï¸âƒ£ CLI Mode (Command Line)
Run the backend directly from the terminal:

```bash
python main.py
```

This will:
- Download the COCO dataset if not available  
- Generate embeddings for images and captions  
- Perform text-based image search via command-line output  

---

### 2ï¸âƒ£ Streamlit Web Interface (Recommended)

Launch the full web app:
```bash
streamlit run app.py
```

Then open your browser:
```
http://localhost:8501
```

The interface includes:
- Tabs for text-to-image, image-to-image, and PDF searches  
- Download buttons for dataset preparation  
- Real-time similarity scoring and visual results display  

---

## â˜ï¸ Deployment Notes

- For **local experiments**, the app runs with the full COCO dataset and user-provided PDFs.  
- For **Streamlit Cloud / Hugging Face Spaces**, use a reduced dataset (e.g., 200â€“500 samples) with precomputed 
embeddings.  
- GPU acceleration is **automatically enabled** when CUDA is available.

---

## ğŸ§­ Current Development Progress

| Phase       | Description                                               | Status         |
|-------------|-----------------------------------------------------------|----------------|
| **Phase 1** | Text & image semantic search (CLIP/M-CLIP)                | âœ… Completed    |
| **Phase 2** | PDF semantic similarity search (text-to-PDF & PDF-to-PDF) | âœ… Completed    |
| **Phase 3** | Audio similarity using Whisper                            | ğŸš§ Not started |
| **Phase 4** | Video content search and indexing                         | ğŸš§ Not started |

---

## ğŸ§ª Example Queries

| Input Type    | Example Query                        | Output                                                  |
|---------------|--------------------------------------|---------------------------------------------------------|
| Text â†’ Image  | â€œA cat on green grassâ€               | Returns top-5 COCO images with cosine similarity scores |
| Image â†’ Image | Upload any photo                     | Finds visually similar images                           |
| PDF â†’ PDF     | Upload a thesis PDF                  | Finds other PDFs with semantically related text         |
| Text â†’ PDF    | â€œNeural networks for classificationâ€ | Locates documents discussing deep learning topics       |

---

## ğŸ§° Installation

### Using Conda
```bash
conda env create -f environment.yml
conda activate content-search-ai
```

### Using pip
```bash
pip install -r requirements.txt
```

---

## ğŸ‘¨â€ğŸ’» Author
**Thesis by:** Nikolaos Psaltakis  
**Institution:** University of West Attica  
**Department:** Computer Science  
**Year:** 2025  

---

## ğŸ“œ License
This project is developed exclusively for **academic and research purposes**.  
Redistribution or commercial use is not permitted without written permission.
