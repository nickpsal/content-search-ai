#!/usr/bin/env python3
"""
AudioSearcher v2 ‚Äì MCLIP Semantic Search + Emotion
---------------------------------------------------
Features:
- Faster-Whisper (CPU) Œ≥ŒπŒ± transcription (ŒºœåŒΩŒø Œ≥ŒπŒ± build/update transcripts)
- M-CLIP (mclip_finetuned_coco_ready) Œ≥ŒπŒ± semantic search œÄŒ¨ŒΩœâ œÉœÑŒ± transcripts
- Emotion Model V5 (best_model_audio_emotion_v5.pt) Œ≥ŒπŒ± emotion detection
- Œ•œÄŒøœÉœÑŒ∑œÅŒØŒ∂ŒµŒπ:
    - build/update transcripts (main + other)
    - build MCLIP embeddings Œ≥ŒπŒ± œåŒªŒ± œÑŒ± transcripts
    - semantic search œÉŒµ EL/EN (Œ∫Œ±Œπ Œ≥ŒµŒΩŒπŒ∫Œ¨ multilingual)
"""

import torch
import librosa
import soundfile as sf
import numpy as np
import pandas as pd
from pathlib import Path
from tqdm import tqdm
from faster_whisper import WhisperModel as FasterWhisper
from sentence_transformers import SentenceTransformer

# ============================
#    EMOTION MODEL IMPORT
# ============================
from .emotion_model_v5 import EmotionModelV5


# ============================
#   SAFE CSV LOADER
# ============================
def _safe_load_csv(path: Path):
    """Load a CSV safely ‚Äì return None if empty or invalid."""
    if not path.exists():
        return None

    if path.stat().st_size == 0:
        print(f"‚ö†Ô∏è Empty transcript file ‚Üí {path}, rebuilding...")
        return None

    try:
        df = pd.read_csv(path)
        if df.empty:
            print(f"‚ö†Ô∏è Transcript file empty ‚Üí {path}, rebuilding...")
            return None
        return df
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to load {path}: {e}, rebuilding...")
        return None


# -------------------------------------------------
# Detect query language (EL / EN)
# -------------------------------------------------
def detect_language(text: str):
    for ch in text:
        if 'Œ±' <= ch <= 'œâ' or 'Œë' <= ch <= 'Œ©':
            return "el"
    return "en"

# ============================
#       AUDIO SEARCHER
# ============================
class AudioSearcher:
    def __init__(
        self,
        audio_main="data/audio/AudioWAV",
        audio_other="data/audio/audio_other",
        emotion_model_path="models/best_model_audio_emotion_v5.pt",
        transcripts_main="data/transcripts/transcripts_main.csv",
        transcripts_other="data/transcripts/transcripts_other.csv",
        mclip_model_path="models/mclip_finetuned_coco_ready",
        device=None,
    ):
        """
        AudioSearcher v2
        - audio_main: Œ≤Œ±œÉŒπŒ∫œå dataset (œÄ.œá. RAVDESS/CREMA-D)
        - audio_other: custom œáœÅŒÆœÉœÑŒ∑ (Œ∑œáŒøŒ≥œÅŒ±œÜŒÆœÉŒµŒπœÇ Œ∫ŒªœÄ)
        - emotion_model_path: path œÉœÑŒø best_model_audio_emotion_v5.pt
        - mclip_model_path: path œÉœÑŒø M-CLIP fine-tuned model (SentenceTransformer)
        """

        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")

        # Paths
        self.audio_main = Path(audio_main)
        self.audio_other = Path(audio_other)
        self.emotion_model_path = Path(emotion_model_path)
        self.transcripts_main_path = Path(transcripts_main)
        self.transcripts_other_path = Path(transcripts_other)
        self.mclip_model_path = mclip_model_path

        # Ensure dirs
        self.transcripts_main_path.parent.mkdir(exist_ok=True, parents=True)
        self.transcripts_other_path.parent.mkdir(exist_ok=True, parents=True)

        # Embeddings directory
        self.emb_dir = Path("data/transcripts/embeds")
        self.emb_dir.mkdir(parents=True, exist_ok=True)

        print("=======================================")
        print(f"üöÄ Device            : {self.device}")
        print(f"üìÇ Audio MAIN        : {self.audio_main}")
        print(f"üìÇ Audio OTHER       : {self.audio_other}")
        print(f"üß† Emotion model     : {self.emotion_model_path}")
        print(f"üî§ M-CLIP model path : {self.mclip_model_path}")
        print("=======================================\n")

        self._load_models()

        # Will be loaded lazy
        self.transcripts = None
        self.transcripts_main = None
        self.transcripts_other = None

    # ==========================================
    #  LOAD MODELS
    # ==========================================
    def _load_models(self):
        print("üîπ Loading Faster-Whisper (CPU transcription)...")
        self.fw = FasterWhisper(
            "small",
            device="cpu",           # ALWAYS CPU (œÉœÑŒ±Œ∏ŒµœÅœå + œáœâœÅŒØœÇ cuDNN Œ∏Œ≠ŒºŒ±œÑŒ±)
            compute_type="int8"     # Fast & lightweight
        )

        print("üîπ Loading Emotion Model V5...")
        self.emotion_model = EmotionModelV5(
            self.emotion_model_path,
            device=self.device
        )

        print("üîπ Loading M-CLIP (SentenceTransformer)...")
        self.mclip = SentenceTransformer(self.mclip_model_path)
        print("‚úÖ All models loaded.\n")

    # ==========================================
    #   LOAD AUDIO (utility)
    # ==========================================
    def load_audio(self, path: Path):
        audio, sr = sf.read(path)
        if audio.ndim > 1:
            audio = audio.mean(axis=1)
        if sr != 16000:
            audio = librosa.resample(audio, orig_sr=sr, target_sr=16000)
        return audio, 16000

    # ==========================================
    #  TRANSCRIBE SINGLE FILE
    # ==========================================
    def transcribe_audio(self, path: Path) -> str:
        segs, _ = self.fw.transcribe(str(path), beam_size=1)
        return " ".join([s.text for s in segs]).strip()

    # ==========================================
    #  BUILD TRANSCRIPTS + MCLIP EMBEDDINGS
    # ==========================================
    def build_all_transcripts(self):
        """
        - Œ¶œÑŒπŒ¨œáŒΩŒµŒπ / œÜŒøœÅœÑœéŒΩŒµŒπ transcripts Œ≥ŒπŒ±:
            - audio_other ‚Üí transcripts_other.csv
            - audio_main  ‚Üí transcripts_main.csv
        - ŒöœÅŒ±œÑŒ¨ŒµŒπ self.transcripts œâœÇ merged DataFrame
        - Œ¶œÑŒπŒ¨œáŒΩŒµŒπ MCLIP embeddings (.npy) Œ≥ŒπŒ± ŒüŒõŒë œÑŒ± transcripts
        """

        # ---------- OTHER ----------
        df_other = _safe_load_csv(self.transcripts_other_path)
        if df_other is None:
            rows = []
            wavs = sorted(self.audio_other.glob("*.wav"))
            if not wavs:
                print(f"‚ÑπÔ∏è No .wav files found in OTHER folder: {self.audio_other}")
            for w in tqdm(wavs, desc="Transcribing OTHER"):
                rows.append({
                    "filename": w.name,
                    "folder": "other",
                    "transcript": self.transcribe_audio(w)
                })
            df_other = pd.DataFrame(rows)
            df_other.to_csv(self.transcripts_other_path, index=False)
        self.transcripts_other = df_other

        # ---------- MAIN ----------
        df_main = _safe_load_csv(self.transcripts_main_path)
        if df_main is None:
            rows = []
            wavs = sorted(self.audio_main.glob("*.wav"))
            if not wavs:
                print(f"‚ÑπÔ∏è No .wav files found in MAIN folder: {self.audio_main}")
            for w in tqdm(wavs, desc="Transcribing MAIN"):
                rows.append({
                    "filename": w.name,
                    "folder": "main",
                    "transcript": self.transcribe_audio(w)
                })
            df_main = pd.DataFrame(rows)
            df_main.to_csv(self.transcripts_main_path, index=False)
        self.transcripts_main = df_main

        # ---------- MERGE ----------
        self.transcripts = pd.concat([df_other, df_main], ignore_index=True)

        # ---------- BUILD MCLIP EMBEDDINGS ----------
        print("\nüîπ Building MCLIP embeddings for all transcripts...\n")
        for _, row in tqdm(
            self.transcripts.iterrows(),
            total=self.transcripts.shape[0],
            desc="MCLIP embeddings"
        ):
            fname = row["filename"]
            emb_path = self.emb_dir / f"{fname}.npy"

            # ŒëŒΩ œÖœÄŒ¨œÅœáŒµŒπ ŒÆŒ¥Œ∑, œÑŒø Œ±œÜŒÆŒΩŒøœÖŒºŒµ (Œ≥œÅŒÆŒ≥ŒøœÅŒø rebuild)
            if emb_path.exists():
                continue

            text = row["transcript"]
            emb = self.mclip.encode(text, normalize_embeddings=True)
            np.save(emb_path, emb)

        print("‚úÖ Transcripts + embeddings ready.\n")
        return self.transcripts

    # ==========================================
    #  MCLIP SEMANTIC SEARCH + EMOTION
    # ==========================================
    def search_semantic_emotion(self, query: str, top_k=10):
        """
        MCLIP Semantic Search + Emotion + Language Penalty
        ---------------------------------------------------
        - embed query with MCLIP
        - compute cosine similarity with transcript embeddings
        - apply language mismatch penalty
        - return top_k most relevant results
        - run Emotion Model V5 ONLY on top_k
        """

        # Ensure transcripts & embeddings exist
        if self.transcripts is None:
            self.build_all_transcripts()

        print(f"\nüîç MCLIP Semantic Search for: '{query}'\n")

        query_lang = detect_language(query)
        print(f"üåê Query language detected: {query_lang.upper()}")

        # -------------------------------------------------
        # Encode query once
        # -------------------------------------------------
        q_emb = self.mclip.encode(query, normalize_embeddings=True)

        results = []

        # -------------------------------------------------
        # Compute similarity with every transcript embedding
        # -------------------------------------------------
        for _, row in tqdm(
                self.transcripts.iterrows(),
                total=self.transcripts.shape[0],
                desc="Computing similarities"
        ):
            fname = row["filename"]
            emb_path = self.emb_dir / f"{fname}.npy"

            if not emb_path.exists():
                continue

            t_emb = np.load(emb_path)

            # Base similarity (cosine)
            sim = float(np.dot(q_emb, t_emb))

            # Detect transcript language
            text_lang = detect_language(row["transcript"])

            # -------------------------------------------------
            # üö® LANGUAGE PENALTY
            # -------------------------------------------------
            if query_lang != text_lang:
                sim -= 1.0  # Strong language penalty

            folder = row["folder"]
            full_path = (
                self.audio_other / fname if folder == "other"
                else self.audio_main / fname
            )

            results.append({
                "filename": fname,
                "folder": folder,
                "full_path": str(full_path),
                "transcript": row["transcript"],
                "similarity": sim,
                "text_language": text_lang
            })

        # -------------------------------------------------
        # Sort by similarity
        # -------------------------------------------------
        results = sorted(results, key=lambda x: x["similarity"], reverse=True)

        # üî• Remove negative scores
        results = [r for r in results if r["similarity"] > 0]

        top_results = results[:top_k]

        # -------------------------------------------------
        # Run Emotion Model only on top_k results
        # -------------------------------------------------
        print("\nüîπ Running Emotion Model on top results...\n")

        for r in top_results:
            emo_label, emo_probs = self.emotion_model.predict(Path(r["full_path"]))
            r["emotion"] = emo_label
            r["emotion_probs"] = emo_probs

        print("\n‚úÖ Semantic search complete.\n")
        return top_results

